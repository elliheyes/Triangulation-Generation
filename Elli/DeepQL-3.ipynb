{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f23d4fc-19bf-4fc7-8efb-03f561610962",
   "metadata": {},
   "source": [
    "Author: Elli Heyes (elli.heyes@city.ac.uk)\n",
    "Last edited: 08/05/2024\n",
    "\n",
    "In this notebook we construct a reinforcement learning algorithm to generate FRST triangulations of reflexive polytopes together with rank-5 line bundle sums that satisfy anomaly cancellation and poly-stability.\n",
    "We use a deep Q-learning algorithm, where the triangulation states are encoded by 2-face triangulations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef9cc4ad-b220-445f-93a3-3621bc3ef52d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.16.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m784.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /home/cytools/cytools-venv/lib/python3.11/site-packages (from tensorflow) (1.4.0)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=23.5.26\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h5py>=3.10.0\n",
      "  Downloading h5py-3.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting libclang>=13.0.0\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting ml-dtypes~=0.3.1\n",
      "  Downloading ml_dtypes-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/cytools/cytools-venv/lib/python3.11/site-packages (from tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/cytools/cytools-venv/lib/python3.11/site-packages (from tensorflow) (4.22.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/cytools/cytools-venv/lib/python3.11/site-packages (from tensorflow) (2.28.2)\n",
      "Requirement already satisfied: setuptools in /home/cytools/cytools-venv/lib/python3.11/site-packages (from tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/cytools/cytools-venv/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/cytools/cytools-venv/lib/python3.11/site-packages (from tensorflow) (4.5.0)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.16.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.7/80.7 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.63.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard<2.17,>=2.16\n",
      "  Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting keras>=3.0.0\n",
      "  Downloading keras-3.3.3-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /home/cytools/cytools-venv/lib/python3.11/site-packages (from tensorflow) (1.24.2)\n",
      "Collecting wheel<1.0,>=0.23.0\n",
      "  Downloading wheel-0.43.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rich\n",
      "  Downloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting namex\n",
      "  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Collecting optree\n",
      "  Downloading optree-0.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (312 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.0/312.0 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /home/cytools/cytools-venv/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/cytools/cytools-venv/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/cytools/cytools-venv/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/cytools/cytools-venv/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2022.12.7)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-3.0.3-py3-none-any.whl (227 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.3/227.3 kB\u001b[0m \u001b[31m944.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.1.1 in /home/cytools/cytools-venv/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.2)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/cytools/cytools-venv/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow) (2.15.1)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, wheel, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, optree, opt-einsum, ml-dtypes, mdurl, markdown, h5py, grpcio, google-pasta, gast, tensorboard, markdown-it-py, astunparse, rich, keras, tensorflow\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.8.0\n",
      "    Uninstalling h5py-3.8.0:\n",
      "      Successfully uninstalled h5py-3.8.0\n",
      "Successfully installed astunparse-1.6.3 flatbuffers-24.3.25 gast-0.5.4 google-pasta-0.2.0 grpcio-1.63.0 h5py-3.11.0 keras-3.3.3 libclang-18.1.1 markdown-3.6 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.3.2 namex-0.0.8 opt-einsum-3.3.0 optree-0.11.0 rich-13.7.1 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.1 tensorflow-io-gcs-filesystem-0.37.0 termcolor-2.4.0 werkzeug-3.0.3 wheel-0.43.0 wrapt-1.16.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /home/cytools/cytools-venv/lib/python3.11/site-packages (3.3.3)\n",
      "Requirement already satisfied: absl-py in /home/cytools/cytools-venv/lib/python3.11/site-packages (from keras) (1.4.0)\n",
      "Requirement already satisfied: numpy in /home/cytools/cytools-venv/lib/python3.11/site-packages (from keras) (1.24.2)\n",
      "Requirement already satisfied: rich in /home/cytools/cytools-venv/lib/python3.11/site-packages (from keras) (13.7.1)\n",
      "Requirement already satisfied: namex in /home/cytools/cytools-venv/lib/python3.11/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: h5py in /home/cytools/cytools-venv/lib/python3.11/site-packages (from keras) (3.11.0)\n",
      "Requirement already satisfied: optree in /home/cytools/cytools-venv/lib/python3.11/site-packages (from keras) (0.11.0)\n",
      "Requirement already satisfied: ml-dtypes in /home/cytools/cytools-venv/lib/python3.11/site-packages (from keras) (0.3.2)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/cytools/cytools-venv/lib/python3.11/site-packages (from optree->keras) (4.5.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/cytools/cytools-venv/lib/python3.11/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/cytools/cytools-venv/lib/python3.11/site-packages (from rich->keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/cytools/cytools-venv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m2024-05-08 08:15:46.148367: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-08 08:15:46.157200: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-08 08:15:46.253389: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-08 08:15:47.517103: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "try:\n",
    "    import keras \n",
    "    import tensorflow as tf\n",
    "except:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'tensorflow'])\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'keras'])\n",
    "    import keras\n",
    "    import tensorflow as tf\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import comb\n",
    "from copy import deepcopy\n",
    "from flint import fmpz_mat\n",
    "from collections import deque\n",
    "from cytools import Polytope\n",
    "from cytools import fetch_polytopes\n",
    "from cytools.cone import Cone\n",
    "from cytools.utils import gcd_list\n",
    "from cytools.polytope import poly_v_to_h\n",
    "from cytools.triangulation import Triangulation\n",
    "from scipy.spatial import ConvexHull\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7acfde24-76fc-4814-8d6d-96854275eb2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_two_face_triangs(poly):\n",
    "    \"\"\" Input: polyotpe\n",
    "        Output: 2d list of all inequivalent 2-face triangulations \"\"\"\n",
    "    two_face_triangs = []\n",
    "    for face in p.faces(2):\n",
    "        Ts = face.as_polytope().all_triangulations(only_regular=True, only_star=False, only_fine=True, include_points_interior_to_facets=True, as_list=True)\n",
    "        Ts_simps = []\n",
    "        for T in Ts:\n",
    "            new_simp = [[p.points_to_labels(face.points()[i]) for i in simp] for simp in T.simps()]\n",
    "            Ts_simps.append(new_simp)\n",
    "        two_face_triangs.append(Ts_simps)\n",
    "    \n",
    "    return two_face_triangs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f41b12e-5a5c-4871-bd17-3455ba0cc21c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def random_state(two_face_triangs, max_num_triangs, h11):\n",
    "    \"\"\" Input: - 2d list of all 2-face triangulations\n",
    "               - maximum number of 2-face triangulations \n",
    "               - h11 value of the CY\n",
    "        Output: 3d bitlist state encoding the triangulation and line bundle sum   \"\"\"\n",
    "    \n",
    "    # determine the dimensions of the state bitlist\n",
    "    dim1 = max(len(two_face_triangs),5)\n",
    "    dim2 = max(max_num_triangs,h11)\n",
    "    \n",
    "    # randomly generate triangulation bitlist\n",
    "    T = [[0 for j in range(dim2)] for i in range(dim1)]\n",
    "    for i in range(len(two_face_triangs)):\n",
    "        j = random.choice(range(len(two_face_triangs[i])))\n",
    "        T[i][j] = 1\n",
    "    \n",
    "    # randomly generate line bundle sum\n",
    "    V = [[0 for j in range(dim2)] for i in range(dim1)]\n",
    "    sums = [0 for i in range(h11)]\n",
    "    for i in range(4):\n",
    "        for j in range(h11):\n",
    "            V[i][j] = random.randint(-5,5)\n",
    "            sums[j] += V[i][j]\n",
    "            \n",
    "    # ensure that the structure group of V is S(U(1))^5 and not smaller\n",
    "    embedding = False\n",
    "    while not embedding:\n",
    "        k5 = [random.randint(-5,5) for i in range(h11)]\n",
    "        new_sums = [sums[i]+k5[i] for i in range(h11)]\n",
    "        if new_sums != [0 for i in range(h11)]:\n",
    "            embedding = True\n",
    "            for i in range(h11):\n",
    "                V[4][i] = k5[i]\n",
    "            \n",
    "    # combine triangulation and line bundle sum\n",
    "    state = [T,V]\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c907206a-6eba-4103-9119-37f247f5fa0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bits_to_simps(triang, two_face_triangs):\n",
    "    \"\"\" Input: - bitlist describing triangulation of 2-faces \n",
    "               - list of all 2-face triangulations\n",
    "        Output: the list of two simplices                    \"\"\"\n",
    "    two_simps = []\n",
    "    for i in range(len(triang)):\n",
    "        for j in range(len(triang[i])):\n",
    "            if triang[i][j] == 1:\n",
    "                two_simps.append(two_face_triangs[i][j])\n",
    "    \n",
    "    return two_simps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aab57b1f-84da-4816-a36c-83aba5871c2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_actions(two_face_triangs, h11):\n",
    "    \"\"\" Input: - 2d list of all 2-face triangulations\n",
    "               - h11 value of the CY\n",
    "        Output: the list of all possible actions on the \n",
    "                triangulation and line bundle sum       \"\"\"\n",
    "    # [i,j]: choose the j-th triangulation of the i-th 2-face\n",
    "    T_actions = []\n",
    "    for i in range(len(two_face_triangs)):\n",
    "        for j in range(len(two_face_triangs[i])):\n",
    "            if len(two_face_triangs[i]) > 1:\n",
    "                T_actions.append([0,i,j,0])\n",
    "    \n",
    "    # [i,j,k]: change the k^i_j value to k\n",
    "    V_actions = []\n",
    "    for i in range(5):\n",
    "        for j in range(h11):\n",
    "            for k in range(-5,6):\n",
    "                V_actions.append([1,i,j,k])\n",
    "    \n",
    "    # combine action lists\n",
    "    action_list = T_actions + V_actions\n",
    "    \n",
    "    return action_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1d35392-9897-4238-b23c-9c600e22127f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def act(state, action):\n",
    "    \"\"\" Input: state and action pair\n",
    "        Output: a new state obtained by acting on the input state with the action \"\"\"\n",
    "    new_state = deepcopy(state)\n",
    "    \n",
    "    if action[0] == 0:\n",
    "        new_state[0][action[1]] = [0 for i in range(len(state[0][action[1]]))]\n",
    "        new_state[0][action[1]][action[2]] = 1\n",
    "        \n",
    "    else:\n",
    "        new_state[1][action[1]][action[2]] = action[3]\n",
    "    \n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00af6fd6-307f-4873-8bfa-441140f4d3fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def secondary_cone(poly, two_face, simps):\n",
    "    \"\"\" Input: points, simplices and dimension\n",
    "        Output: secondary cone of polytope \"\"\"\n",
    "    face = Polytope(two_face.points())\n",
    "    \n",
    "    two_pts_ext = [list(pt)+[1,] for pt in face.points(optimal=True)]\n",
    "    pts_ext = [list(pt)+[1,] for pt in poly.points(optimal=True)]\n",
    "    \n",
    "    two_simps = [[face.points_to_labels(poly.points()[simp[i]]) for i in range(3)] for simp in simps]\n",
    "    two_simps = [set(s) for s in two_simps]\n",
    "    \n",
    "    simps = [set(s) for s in simps]\n",
    "    \n",
    "    m = np.zeros((2+1, 2+2), dtype=int)\n",
    "    null_vecs = set()\n",
    "    for i in range(len(simps)):\n",
    "        for j in range(len(simps[i+1:])):\n",
    "            # define the simplices\n",
    "            two_s1 = two_simps[i]\n",
    "            two_s2 = two_simps[j]\n",
    "            s1 = simps[i]\n",
    "            s2 = simps[j]\n",
    "\n",
    "            # eunsre that the simplices have a large enough intersection\n",
    "            two_comm_pts = two_s1 & two_s2\n",
    "            if len(two_comm_pts) != 2:\n",
    "                continue\n",
    " \n",
    "            two_diff_pts = list(two_s1 ^ two_s2)\n",
    "            two_comm_pts = list(two_comm_pts)\n",
    "            for k,pt in enumerate(two_diff_pts):    m[:,k] = two_pts_ext[pt]\n",
    "            for k,pt in enumerate(two_comm_pts):    m[:,k+2] = two_pts_ext[pt]\n",
    "        \n",
    "            # calculate nullspace/hyperplane inequality\n",
    "            v = fmpz_mat(m.tolist()).nullspace()[0]\n",
    "            v = np.array(v.transpose().tolist()[0], dtype=int)\n",
    "\n",
    "            # ensure the sign is correct\n",
    "            if v[0] < 0:\n",
    "                v *= -1\n",
    "\n",
    "            # reduce the vector\n",
    "            g = gcd_list(v)\n",
    "            if g != 1:\n",
    "                v //= g\n",
    "\n",
    "            # construct the full vector (including all points)\n",
    "            full_v = np.zeros(len(pts_ext), dtype=int)\n",
    "\n",
    "            diff_pts = list(s1 ^ s2)\n",
    "            comm_pts = list(s1 & s2)\n",
    "            for k,pt in enumerate(diff_pts):    full_v[pt] = v[k]\n",
    "            for k,pt in enumerate(comm_pts):    full_v[pt] = v[k+2]\n",
    "\n",
    "            full_v = tuple(full_v)\n",
    "            if full_v not in null_vecs:\n",
    "                null_vecs.add(full_v)\n",
    "    \n",
    "    if null_vecs == set():\n",
    "        sec_cone = None\n",
    "    else:         \n",
    "        sec_cone = Cone(hyperplanes=list(null_vecs),check=False)\n",
    "    \n",
    "    return sec_cone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90cd31c5-8595-4bb7-a8f9-5a775d126ad8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2):\n",
    "    \"\"\" Input: two lists\n",
    "        Output: the intersection of the two lists \"\"\"\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    return lst3\n",
    "\n",
    "def complement(lst1, lst2):\n",
    "    \"\"\" Input: two lists\n",
    "        Output: the complement of the two lists \"\"\"\n",
    "    lst3 = []\n",
    "    for item in lst1:\n",
    "        if not item in lst2:\n",
    "            lst3.append(item)\n",
    "    for item in lst2:\n",
    "        if not item in lst1:\n",
    "            lst3.append(item)\n",
    "   \n",
    "    return lst3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0c81dd9-17b7-40b4-ae40-e4485b463ada",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def combine_triangulation(poly, two_face_triangs, bitlist):\n",
    "    \"\"\" Input: a polytope, the list of 2-face triangulations and the bitlist state\n",
    "        Output: the combined triangulation                                         \"\"\"\n",
    "    \n",
    "    # get the list of facets\n",
    "    facets = []\n",
    "    for i in range(len(poly.facets())):\n",
    "        facets.append(poly.points_to_labels(poly.facets()[i].points()))\n",
    "        \n",
    "    # get the list of 2-face simplices\n",
    "    two_face_simps = [item for sublist in bits_to_simps(bitlist, two_face_triangs) for item in sublist]\n",
    "    \n",
    "    # combine the simplices from the 2-face triangulations \n",
    "    simps = []\n",
    "    for i in range(len(two_face_simps)):\n",
    "        for j in range(i,len(two_face_simps)):\n",
    "            if i!=j:\n",
    "                inters = intersection(two_face_simps[i], two_face_simps[j])\n",
    "                comp = complement(two_face_simps[i], two_face_simps[j])\n",
    "                if len(inters) == 2:\n",
    "                    comp = complement(two_face_simps[i], two_face_simps[j])\n",
    "                    for k in range(len((two_face_simps))):\n",
    "                        if np.array([item in two_face_simps[k] for item in comp]).all():\n",
    "                            simp = sorted(inters + comp)\n",
    "                            for facet in facets:\n",
    "                                if np.array([item in facet for item in simp]).all():\n",
    "                                    if not simp in simps:\n",
    "                                        simps.append(simp)\n",
    "    \n",
    "    simps = [[0]+simp for simp in simps]\n",
    " \n",
    "    return Triangulation(poly=poly, pts=poly._labels_not_facet, simplices=simps, check_input_simplices=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e71ff492-062f-477c-a592-ad124fc56a7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def R(state, two_face_triangs, h11, poly): \n",
    "    \"\"\" Input: - state describing triangulation and line bundle sum\n",
    "               - list of two face simplices\n",
    "               - h11 value of the CY\n",
    "               - reflexive polytope\n",
    "        Output: the reward value between 0 and 1 for the state \"\"\"\n",
    "    \n",
    "    # covert triangulation bitlist to list of two-face simplices\n",
    "    two_face_simps = bits_to_simps(state[0], two_face_triangs)\n",
    "        \n",
    "    # compute the secondary cones of all the non-simplicial two-face triangulations\n",
    "    sec_cones = []\n",
    "    for i in range(len(two_face_simps)):\n",
    "        if len(two_face_simps[i]) > 1:\n",
    "            sec_cones.append(secondary_cone(p, p.faces(2)[i], two_face_simps[i]))\n",
    "    sec_cones = list(set(sec_cones))\n",
    "    \n",
    "    # check if cone is trivial\n",
    "    if sec_cones != [None]:\n",
    "        # check the regularity condition\n",
    "        inters = None\n",
    "        for i in range(len(sec_cones)):\n",
    "            if sec_cones[i] != None:\n",
    "                if inters == None:\n",
    "                    inters = sec_cones[i]\n",
    "                else:\n",
    "                    inters = inters.intersection(sec_cones[i])\n",
    "        if inters.is_solid():\n",
    "            FRST = 1\n",
    "        else:\n",
    "            FRST = 0\n",
    "    else:\n",
    "        FRST = 1\n",
    "    \n",
    "    # if FRST check E8 embedding condition \n",
    "    if FRST:\n",
    "        embeddings = [0 for i in range(h11)]\n",
    "        for i in range(h11):\n",
    "            if state[1][0][i]+state[1][1][i]+state[1][2][i]+state[1][3][i]+state[1][4][i] == 0:\n",
    "                embeddings[i] = 1\n",
    "            \n",
    "        # if embedding condition satisfied check anomaly cancellation condition\n",
    "        if sum(embeddings) == 0:\n",
    "            # combine the two-face simplices into a triangulation of the full polytope\n",
    "            t = combine_triangulation(poly, two_face_triangs, state[0])\n",
    "        \n",
    "            # define the CY\n",
    "            cy = t.get_cy()\n",
    "        \n",
    "            # compute the second Chern class and triple intersection numbers\n",
    "            c2 = cy.second_chern_class(in_basis=True)\n",
    "            dijk = cy.intersection_numbers(in_basis=True, format=\"dense\")\n",
    "         \n",
    "            anomalies = [0 for i in range(h11)]\n",
    "            for i in range(h11):\n",
    "                c2iV = 0\n",
    "                for a in range(5):\n",
    "                    for j in range(h11):\n",
    "                        for k in range(h11):\n",
    "                            c2iV += -0.5*dijk[i][j][k]*state[1][a][j]*state[1][a][k]\n",
    "                if c2iV <= c2[i]:\n",
    "                    anomalies[i] = 1\n",
    "                    \n",
    "            # if anomaly cancellation condition satisfied check poly-stability condition\n",
    "            if sum(anomalies) == 0:\n",
    "                \n",
    "                # define the 5 M matrices\n",
    "                Ms = [[dijk[i]*state[1][a][i] for i in range(h11)] for a in range(5)]\n",
    "                \n",
    "                # check the poly-stability condition \n",
    "                stabilities = [0 for a in range(5)]\n",
    "                for a in range(5):\n",
    "                    if not (max(np.array(Ms[a]).flatten()) > 0 and min(np.array(Ms[a]).flatten()) < 0):\n",
    "                        stabilities[a] = 1\n",
    "                \n",
    "                if sum(stabilities) == 0:\n",
    "                    score = 1\n",
    "                else:\n",
    "                    score = 1 - (1/3)*sum(stabilities)/5\n",
    "            \n",
    "            else:\n",
    "                score = (2/3) - (1/3)*sum(anomalies)/h11 \n",
    "                \n",
    "        else:\n",
    "            score = (1/3) - (1/3)*sum(embeddings)/h11\n",
    "            \n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9146cd83-072c-492f-9a17-37d9978ed7db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def agent(state_shape, action_shape):\n",
    "    \"\"\" The agent maps X-states to Y-actions.\"\"\"\n",
    "    learning_rate = 0.001\n",
    "    my_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    my_loss = tf.keras.losses.MeanSquaredError()\n",
    "    my_metric = tf.keras.metrics.MeanAbsoluteError() \n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(100, input_shape=state_shape, activation='relu'))\n",
    "    model.add(keras.layers.Dense(200, activation='relu'))\n",
    "    model.add(keras.layers.Dense(action_shape, activation='linear'))\n",
    "    model.compile(loss=my_loss, optimizer=my_optimizer, metrics=[my_metric])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abd33d94-8e82-4281-a483-ea32570b5320",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(replay_memory, model, target_model, terminated):\n",
    "    learning_rate = 0.7 \n",
    "    discount_factor = 0.618\n",
    "\n",
    "    min_replay_size = 1000\n",
    "    if len(replay_memory)<min_replay_size:\n",
    "        return\n",
    "\n",
    "    batch_size = 64*2\n",
    "    mini_batch = random.sample(replay_memory, batch_size)\n",
    "    current_states = np.array([np.array(transition[0]).flatten() for transition in mini_batch])\n",
    "    current_qs_list = model.predict(current_states, verbose=0)\n",
    "    new_current_states = np.array([np.array(transition[3]).flatten() for transition in mini_batch])\n",
    "    future_qs_list = target_model.predict(new_current_states, verbose=0)\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "    for index, (state, action, reward, new_state, terminated) in enumerate(mini_batch):\n",
    "        if not terminated:\n",
    "            max_future_q = reward+discount_factor*np.max(future_qs_list[index])\n",
    "        else:\n",
    "            max_future_q = reward\n",
    "\n",
    "        current_qs = current_qs_list[index]\n",
    "        current_qs[action] = (1-learning_rate)*current_qs[action]+learning_rate*max_future_q\n",
    "\n",
    "        X.append(np.array(state).flatten())\n",
    "        Y.append(current_qs)\n",
    "\n",
    "    model.fit(np.array(X), np.array(Y), batch_size=batch_size, verbose=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d38b56ab-75ec-4283-8727-3355174a3491",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "# retreive the list of reflexive polyoptes from KS\n",
    "h11 = 2\n",
    "all_polys = fetch_polytopes(h11=h11, lattice=\"N\", as_list=True, limit=10000)\n",
    "print(len(all_polys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f22cfb1b-db87-475b-9b5a-2dd4e24d5e66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set the netowrk hyperparameters\n",
    "max_epsilon = 1 # you can't explore more than 100% of the time\n",
    "min_epsilon = 0.01 # at a minimum, we'll always explore 1% of the time\n",
    "decay = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8439d986-694a-4af1-b758-38a3875866da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max # 2-Face Triangulations:  2\n"
     ]
    }
   ],
   "source": [
    "# define the reflexive polytopes \n",
    "p = all_polys[3]\n",
    "\n",
    "# get the list of all 2-face triangulations \n",
    "two_face_triangs =get_two_face_triangs(p)\n",
    "    \n",
    "# get the maximum number of triangulations for a 2-face\n",
    "max_num_triangs = max(len(x) for x in two_face_triangs)\n",
    "print(\"Max # 2-Face Triangulations: \",max_num_triangs)\n",
    "\n",
    "# get the action list \n",
    "action_list = get_actions(two_face_triangs, h11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95f03c41-9789-4a88-ab46-4c5933cf3bc4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim1:  13\n",
      "Dim2:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cytools/cytools-venv/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "dim1 = max(len(two_face_triangs),5)\n",
    "dim2 = max(max_num_triangs,h11)\n",
    "print(\"Dim1: \",dim1)\n",
    "print(\"Dim2: \",dim2)\n",
    "\n",
    "state_shape =  (2*dim1*dim2,)\n",
    "action_shape = len(action_list)\n",
    "\n",
    "# initialise the main model (updated every 4 steps)\n",
    "model = agent(state_shape, action_shape)\n",
    "\n",
    "# initialise the target model (updated every 100 steps)\n",
    "target_model = agent(state_shape, action_shape)\n",
    "target_model.set_weights(model.get_weights())\n",
    "\n",
    "# initialise memory\n",
    "replay_memory = deque(maxlen=50_000)\n",
    "\n",
    "# epsilon-greedy algorithm in initialized at 1 meaning every step is random at the start\n",
    "epsilon = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d5de09-e90d-4126-9923-64c51ba5c076",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Num Epochs:  101\n",
      "Total Num Epochs:  508\n",
      "Total Num Epochs:  118\n",
      "Total Num Epochs:  30\n",
      "Total Num Epochs:  117\n",
      "Total Num Epochs:  240\n"
     ]
    }
   ],
   "source": [
    "# train the deep Q-learning network for 100 episodes\n",
    "steps_to_update_target_model = 0\n",
    "for episode in range(100):   \n",
    "    # randomly set the initial state\n",
    "    state = random_state(two_face_triangs, max_num_triangs, h11)\n",
    "            \n",
    "    # set the initial epoch number \n",
    "    epochs = 0\n",
    "    \n",
    "    # continue until terminated\n",
    "    terminated = False\n",
    "    while not terminated:\n",
    "        \n",
    "        # update steps to update target model variable\n",
    "        steps_to_update_target_model += 1\n",
    "        \n",
    "        # randomly sample a number from 0 to 1 \n",
    "        # if number is less than epsilon then explore the action space\n",
    "        # else exploit the learned values\n",
    "        if random.uniform(0,1)<epsilon:\n",
    "            # randomly sample an action from the action space\n",
    "            action = random.choice(range(len(action_list)))\n",
    "        else:\n",
    "            # choose action with the highest predicted q value\n",
    "            encoded = np.array(state).flatten()\n",
    "            encoded_reshaped = encoded.reshape([1, encoded.shape[0]])\n",
    "            predicted = model.predict(encoded_reshaped, verbose=0).flatten()\n",
    "            action = np.argmax(predicted)\n",
    "\n",
    "        # define the next state from the action\n",
    "        new_state = act(state, action_list[action])\n",
    "        \n",
    "        # compute the fitness of the next state \n",
    "        fitness = R(new_state, two_face_triangs, h11, p)\n",
    "        \n",
    "        # if the new state is FRST then assign a big reward and update the terminated variable\n",
    "        if fitness == 1:\n",
    "            reward = 10\n",
    "            terminated = True\n",
    "        else:\n",
    "            # if the new state is not FRST then compute the reward using the reward function\n",
    "            reward = R(new_state, two_face_triangs, h11, p) - R(state, two_face_triangs, h11, p)   \n",
    "        \n",
    "        # update the replay memory \n",
    "        replay_memory.append([state, action, reward, new_state, terminated])\n",
    "        \n",
    "        # update the main model\n",
    "        if steps_to_update_target_model % 4 == 0 or terminated:\n",
    "            train(replay_memory, model, target_model, terminated)\n",
    "        \n",
    "        # update the state\n",
    "        state = deepcopy(new_state)\n",
    "        \n",
    "        if terminated:\n",
    "            if steps_to_update_target_model >= 100:\n",
    "                # update the target model\n",
    "                target_model.set_weights(model.get_weights())\n",
    "                steps_to_update_target_model = 0\n",
    "                \n",
    "        # update epoch number\n",
    "        epochs += 1\n",
    "    \n",
    "    # update epsilon parameter\n",
    "    epsilon = min_epsilon+(max_epsilon-min_epsilon)*np.exp(-decay*episode)\n",
    "    \n",
    "    print(\"Total Num Epochs: \",epochs)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "e9f4f93d-ebc1-4bc6-a436-2c6a16d8f4c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Num Epochs:  49\n",
      "Total Num Epochs:  8\n",
      "Total Num Epochs:  6\n",
      "Total Num Epochs:  16\n",
      "Total Num Epochs:  1\n",
      "Total Num Epochs:  5\n",
      "Total Num Epochs:  8\n",
      "Total Num Epochs:  3\n",
      "Total Num Epochs:  5\n",
      "Total Num Epochs:  6\n",
      "Total Num Epochs:  9\n",
      "Total Num Epochs:  12\n",
      "Total Num Epochs:  3\n",
      "Total Num Epochs:  24\n",
      "Total Num Epochs:  3\n",
      "Total Num Epochs:  40\n",
      "Total Num Epochs:  9\n",
      "Total Num Epochs:  12\n",
      "Total Num Epochs:  10\n",
      "Total Num Epochs:  95\n",
      "Total Num Epochs:  18\n",
      "Total Num Epochs:  12\n",
      "Total Num Epochs:  2\n",
      "Total Num Epochs:  72\n",
      "Total Num Epochs:  11\n",
      "Total Num Epochs:  5\n",
      "Total Num Epochs:  3\n",
      "Total Num Epochs:  19\n",
      "Total Num Epochs:  6\n",
      "Total Num Epochs:  30\n",
      "Total Num Epochs:  8\n",
      "Total Num Epochs:  1\n",
      "Total Num Epochs:  14\n",
      "Total Num Epochs:  7\n",
      "Total Num Epochs:  2\n",
      "Total Num Epochs:  14\n",
      "Total Num Epochs:  5\n",
      "Total Num Epochs:  77\n",
      "Total Num Epochs:  5\n",
      "Total Num Epochs:  16\n",
      "Total Num Epochs:  2\n",
      "Total Num Epochs:  3\n",
      "Total Num Epochs:  16\n",
      "Total Num Epochs:  59\n",
      "Total Num Epochs:  3\n",
      "Total Num Epochs:  10\n",
      "Total Num Epochs:  15\n",
      "Total Num Epochs:  8\n",
      "Total Num Epochs:  14\n",
      "Total Num Epochs:  17\n",
      "Total Num Epochs:  22\n",
      "Total Num Epochs:  9\n",
      "Total Num Epochs:  4\n",
      "Total Num Epochs:  4\n",
      "Total Num Epochs:  19\n",
      "Total Num Epochs:  26\n",
      "Total Num Epochs:  17\n",
      "Total Num Epochs:  118\n",
      "Total Num Epochs:  14\n",
      "Total Num Epochs:  23\n",
      "Total Num Epochs:  92\n",
      "Total Num Epochs:  5\n",
      "Total Num Epochs:  6\n",
      "Total Num Epochs:  2\n",
      "Total Num Epochs:  4\n",
      "Total Num Epochs:  15\n",
      "Total Num Epochs:  41\n",
      "Total Num Epochs:  8\n",
      "Total Num Epochs:  8\n",
      "Total Num Epochs:  4\n",
      "Total Num Epochs:  3\n",
      "Total Num Epochs:  1\n",
      "Total Num Epochs:  19\n",
      "Total Num Epochs:  12\n",
      "Total Num Epochs:  61\n",
      "Total Num Epochs:  14\n",
      "Total Num Epochs:  26\n",
      "Total Num Epochs:  5\n",
      "Total Num Epochs:  34\n",
      "Total Num Epochs:  1\n",
      "Total Num Epochs:  18\n",
      "Total Num Epochs:  7\n",
      "Total Num Epochs:  3\n",
      "Total Num Epochs:  8\n",
      "Total Num Epochs:  34\n",
      "Total Num Epochs:  25\n",
      "Total Num Epochs:  7\n",
      "Total Num Epochs:  9\n",
      "Total Num Epochs:  3\n",
      "Total Num Epochs:  3\n",
      "Total Num Epochs:  18\n",
      "Total Num Epochs:  7\n",
      "Total Num Epochs:  12\n",
      "Total Num Epochs:  3\n",
      "Total Num Epochs:  190\n",
      "Total Num Epochs:  6\n",
      "Total Num Epochs:  2\n",
      "Total Num Epochs:  1\n",
      "Total Num Epochs:  2\n",
      "Total Num Epochs:  12\n",
      "Total Num Epochs:  9\n"
     ]
    }
   ],
   "source": [
    "# run the trained network to find FRST and line bundle sum pairs until no new pairs are found in 10 runs\n",
    "finished = False\n",
    "terminal_states, count = [], []\n",
    "while not finished:\n",
    "    # randomly set the initial state\n",
    "    state = random_state(two_face_triangs, max_num_triangs, h11)\n",
    "    \n",
    "    # set the initial epoch number \n",
    "    epochs = 0\n",
    "    \n",
    "    # define the path list of states visited\n",
    "    path = []\n",
    "\n",
    "    # continue until terminated\n",
    "    terminated = False\n",
    "    while not terminated:\n",
    "        # append the path with the current state\n",
    "        path.append(state)\n",
    "        \n",
    "        # choose the next state as the one with the highest q value that hasn't been visited before\n",
    "        new = False\n",
    "        encoded = np.array(state).flatten()\n",
    "        encoded_reshaped = encoded.reshape([1, encoded.shape[0]])\n",
    "        q_list = target_model.predict(encoded_reshaped, verbose=0).flatten()\n",
    "\n",
    "        while not new:\n",
    "            # update epoch number\n",
    "            epochs += 1\n",
    "            \n",
    "            # choose action with the highest predicted q value\n",
    "            action = np.argmax(q_list)\n",
    "\n",
    "            # define the next state from the action\n",
    "            new_state = act(state, action_list[action])\n",
    "            \n",
    "            # check if the next state has been visited before\n",
    "            if new_state not in path:\n",
    "                new = True\n",
    "            \n",
    "            # update the q value of the next state in the q list\n",
    "            q_list[np.argmax(q_list)] = -100\n",
    "            \n",
    "        # check if the next state is FRST with anomaly cancellation satisfied and if so update the terminated variable\n",
    "        reward = R(new_state, two_face_triangs, h11, p)\n",
    "        if reward == 1:\n",
    "            print(\"Total Num Epochs: \",epochs)\n",
    "            terminated = True\n",
    "            count.append(len(terminal_states))\n",
    "            if not new_state in terminal_states:\n",
    "                terminal_states.append(new_state)\n",
    "            else:\n",
    "                \"REPEAT\"\n",
    "                \n",
    "        # update the state\n",
    "        state = deepcopy(new_state)\n",
    "        \n",
    "    # if number of unique terminal states has not increased in the last 100 runs then finish\n",
    "    if len(count) > 100:\n",
    "        finished = True\n",
    "    else:\n",
    "        if len(count) > 10:\n",
    "            if len(list(set(count[-10:]))) == 1:\n",
    "                finished = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "94060bfc-87b4-48ae-a310-f237b14a3ed2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def state_to_CY_V(state, poly, two_face_triangs, h11):\n",
    "    T = combine_triangulation(poly, two_face_triangs, state[0])\n",
    "    CY = T.get_cy()\n",
    "    V = [[state[1][a][i] for i in range(h11)] for a in range(5)]\n",
    "    return CY, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "2bdf89e4-b0d4-4da9-869a-5b23a7a5890c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CYsVs = [state_to_CY_V(state, p, two_face_triangs, h11) for state in terminal_states]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "068fa924-ee93-4d7b-973c-f8bc89bc5701",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(A Calabi-Yau 3-fold hypersurface with h11=2 and h21=74 in a 4-dimensional toric variety,\n",
       " [[4, -5], [5, -5], [4, -3], [-5, 5], [-5, 5]])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CYsVs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfbe7e6-9596-436c-a101-4575baf95abe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
